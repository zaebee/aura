{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# \ud83d\udc1d Aura Node: Interactive Worker\n",
    "\n",
    "This notebook allows you to run an Aura Inference Node on Google Colab using Ollama and Gradio.\n",
    "\n",
    "**Disclaimer:** For Research & Development Use Only. Do not use for commercial hosting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "setup-cell"
   },
   "outputs": [],
   "source": [
    "# @title 1. Install Dependencies\n",
    "!curl -fsSL https://ollama.com/install.sh | sh\n",
    "!pip install gradio requests toml\n",
    "\n",
    "# Download frpc\n",
    "import os\n",
    "if not os.path.exists('frpc'):\n",
    "    print(\"Downloading frpc...\")\n",
    "    !curl -L https://github.com/fatedier/frp/releases/download/v0.61.0/frp_0.61.0_linux_amd64.tar.gz -o frp.tar.gz\n",
    "    !tar -xzf frp.tar.gz\n",
    "    !cp frp_0.61.0_linux_amd64/frpc .\n",
    "    !chmod +x frpc\n",
    "    !rm -rf frp.tar.gz frp_0.61.0_linux_amd64\n",
    "print(\"Setup complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ui-cell"
   },
   "outputs": [],
   "source": [
    "# @title 2. Launch Aura Node UI\n",
    "import os\n",
    "import subprocess\n",
    "import threading\n",
    "import time\n",
    "import requests\n",
    "import gradio as gr\n",
    "import sys\n",
    "import collections\n",
    "import toml\n",
    "from io import StringIO\n",
    "\n",
    "FRP_SERVER_ADDR = \"aura.zae.life\"\n",
    "FRP_SERVER_PORT = 7000\n",
    "\n",
    "# Logging system to redirect stdout to Gradio\n",
    "class Logger:\n",
    "    def __init__(self):\n",
    "        # Keep the last 1000 chunks of log data\n",
    "        self.logs = collections.deque(maxlen=1000)\n",
    "        self.lock = threading.Lock()\n",
    "\n",
    "    def write(self, data):\n",
    "        with self.lock:\n",
    "            self.logs.append(data)\n",
    "\n",
    "    def get_logs(self):\n",
    "        with self.lock:\n",
    "            return \"\".join(self.logs)\n",
    "\n",
    "logger = Logger()\n",
    "\n",
    "class StreamToLogger:\n",
    "    def __init__(self, stream, logger):\n",
    "        self.stream = stream\n",
    "        self.logger = logger\n",
    "\n",
    "    def write(self, data):\n",
    "        self.stream.write(data)\n",
    "        self.logger.write(data)\n",
    "\n",
    "    def flush(self):\n",
    "        self.stream.flush()\n",
    "\n",
    "# Prevent duplicate wrapping\n",
    "if not isinstance(sys.stdout, StreamToLogger):\n",
    "    sys.stdout = StreamToLogger(sys.stdout, logger)\n",
    "    sys.stderr = StreamToLogger(sys.stderr, logger)\n",
    "\n",
    "class AuraNode:\n",
    "    def __init__(self):\n",
    "        self.ollama_process = None\n",
    "        self.frpc_process = None\n",
    "        self.status = \"Idle\"\n",
    "        self.requests_processed = 0\n",
    "        self.is_running = False\n",
    "        self.lock = threading.Lock()\n",
    "\n",
    "    def start(self, model, name, token, remote_port, progress=gr.Progress()):\n",
    "        with self.lock:\n",
    "            if self.is_running:\n",
    "                return \"Already running\"\n",
    "            self.is_running = True\n",
    "            self.status = \"Processing\"\n",
    "        \n",
    "        try:\n",
    "            # 1. Start Ollama\n",
    "            progress(0, desc=\"Starting Ollama server...\")\n",
    "            print(\"--- Starting Ollama server ---\")\n",
    "            if self.ollama_process is None:\n",
    "                self.ollama_process = subprocess.Popen([\"ollama\", \"serve\"], stdout=subprocess.PIPE, stderr=subprocess.STDOUT, text=True)\n",
    "                threading.Thread(target=self._read_output, args=(self.ollama_process,), daemon=True).start()\n",
    "\n",
    "            # Wait for ollama to be up\n",
    "            for i in range(30):\n",
    "                try:\n",
    "                    requests.get(\"http://localhost:11434\")\n",
    "                    break\n",
    "                except requests.exceptions.RequestException:\n",
    "                    time.sleep(1)\n",
    "                    progress(i/30, desc=f\"Waiting for Ollama ({i}s)\")\n",
    "            else:\n",
    "                with self.lock:\n",
    "                    self.is_running = False\n",
    "                    self.status = \"Error\"\n",
    "                return \"Ollama failed to start\"\n",
    "\n",
    "            # 2. Pull model\n",
    "            progress(0.1, desc=f\"Pulling model {model}...\")\n",
    "            print(f\"--- Pulling model: {model} ---\")\n",
    "            pull_proc = subprocess.Popen([\"ollama\", \"pull\", model], stdout=subprocess.PIPE, stderr=subprocess.STDOUT, text=True)\n",
    "            for line in pull_proc.stdout:\n",
    "                print(line, end=\"\")\n",
    "            pull_proc.wait()\n",
    "            \n",
    "            if pull_proc.returncode != 0:\n",
    "                with self.lock:\n",
    "                    self.is_running = False\n",
    "                    self.status = \"Error\"\n",
    "                return f\"Failed to pull {model}\"\n",
    "\n",
    "            # 3. Start frpc\n",
    "            progress(0.9, desc=\"Connecting to Hive via frpc...\")\n",
    "            self._start_frpc(name, token, remote_port)\n",
    "            \n",
    "            with self.lock:\n",
    "                self.status = \"Connected\"\n",
    "            print(\"--- Aura Node is now Connected to the Hive! ---\")\n",
    "            return \"Node Connected!\"\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Error: {e}\")\n",
    "            with self.lock:\n",
    "                self.is_running = False\n",
    "                self.status = \"Error\"\n",
    "            return f\"Error: {e}\"\n",
    "\n",
    "    def _start_frpc(self, name, token, remote_port):\n",
    "        config_data = {\n",
    "            \"serverAddr\": FRP_SERVER_ADDR,\n",
    "            \"serverPort\": FRP_SERVER_PORT,\n",
    "            \"auth\": {\n",
    "                \"method\": \"token\",\n",
    "                \"token\": token\n",
    "            },\n",
    "            \"proxies\": [{\n",
    "                \"name\": name,\n",
    "                \"type\": \"tcp\",\n",
    "                \"localIP\": \"127.0.0.1\",\n",
    "                \"localPort\": 11434,\n",
    "                \"remotePort\": int(remote_port)\n",
    "            }]\n",
    "        }\n",
    "        \n",
    "        with open(\"frpc.toml\", \"w\") as f:\n",
    "            toml.dump(config_data, f)\n",
    "        \n",
    "        print(f\"--- Starting frpc tunnel for {name} on remote port {remote_port} ---\")\n",
    "        if self.frpc_process:\n",
    "            self.frpc_process.terminate()\n",
    "            \n",
    "        self.frpc_process = subprocess.Popen([\"./frpc\", \"-c\", \"frpc.toml\"], stdout=subprocess.PIPE, stderr=subprocess.STDOUT, text=True)\n",
    "        threading.Thread(target=self._read_output, args=(self.frpc_process,), daemon=True).start()\n",
    "\n",
    "    def _read_output(self, process):\n",
    "        for line in process.stdout:\n",
    "            print(line, end=\"\")\n",
    "            # Increment stats on successful inference requests\n",
    "            with self.lock:\n",
    "                if \"POST /api/generate\" in line or \"POST /api/chat\" in line:\n",
    "                    self.requests_processed += 1\n",
    "                if \"login to server success\" in line:\n",
    "                     self.status = \"Connected\"\n",
    "\n",
    "    def stop(self):\n",
    "        print(\"--- Stopping Aura Node ---\")\n",
    "        if self.frpc_process:\n",
    "            self.frpc_process.terminate()\n",
    "            self.frpc_process = None\n",
    "        if self.ollama_process:\n",
    "            self.ollama_process.terminate()\n",
    "            self.ollama_process = None\n",
    "        \n",
    "        with self.lock:\n",
    "            self.is_running = False\n",
    "            self.status = \"Idle\"\n",
    "        return \"Node stopped\"\n",
    "\n",
    "node = AuraNode()\n",
    "\n",
    "with gr.Blocks(title=\"Aura Node\", theme=gr.themes.Soft()) as demo:\n",
    "    gr.Markdown(\"# \ud83d\udc1d Aura Interactive Worker Node\")\n",
    "    gr.Markdown(\"**Disclaimer:** For Research & Development Use Only. Do not use for commercial hosting.\")\n",
    "    \n",
    "    with gr.Row():\n",
    "        with gr.Column(scale=1):\n",
    "            status_label = gr.Label(value=node.status, label=\"Brain Status\")\n",
    "            stats_box = gr.Number(value=node.requests_processed, label=\"Requests Processed\")\n",
    "        with gr.Column(scale=2):\n",
    "            with gr.Row():\n",
    "                model_input = gr.Textbox(value=\"mistral\", label=\"Ollama Model\")\n",
    "                worker_name_input = gr.Textbox(value=\"aura-worker-colab\", label=\"Worker Name\")\n",
    "            with gr.Row():\n",
    "                remote_port_input = gr.Textbox(value=\"8083\", label=\"Remote Port\")\n",
    "                token_input = gr.Textbox(label=\"Hive FRP Token\", type=\"password\")\n",
    "\n",
    "    with gr.Row():\n",
    "        start_btn = gr.Button(\"Start Node\", variant=\"primary\")\n",
    "        stop_btn = gr.Button(\"Stop Node\", variant=\"stop\")\n",
    "\n",
    "    log_output = gr.Textbox(lines=15, label=\"Agent Thinking / Logs\", interactive=False)\n",
    "    \n",
    "    # Refresh logic for UI\n",
    "    timer = gr.Timer(2)\n",
    "    def refresh():\n",
    "        with node.lock:\n",
    "            status = node.status\n",
    "            requests_processed = node.requests_processed\n",
    "        logs = logger.get_logs()\n",
    "        return status, requests_processed, logs\n",
    "    timer.tick(refresh, outputs=[status_label, stats_box, log_output])\n",
    "\n",
    "    start_btn.click(node.start, inputs=[model_input, worker_name_input, token_input, remote_port_input], outputs=[log_output])\n",
    "    stop_btn.click(node.stop, outputs=[log_output])\n",
    "\n",
    "demo.launch(share=True, inline=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
