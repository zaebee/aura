{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# üêù Aura Node: Interactive Worker\n",
    "\n",
    "This notebook allows you to run an Aura Inference Node on Google Colab using Ollama and Gradio.\n",
    "\n",
    "**Disclaimer:** For Research & Development Use Only. Do not use for commercial hosting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "setup-cell"
   },
   "outputs": [],
   "source": [
    "# @title 1. Install Dependencies\n",
    "# NOTE: Piping curl to sh carries security risks. Review scripts before execution.\n",
    "!curl -fsSL https://ollama.com/install.sh | sh\n",
    "!pip install gradio requests toml\n",
    "\n",
    "# Download and verify frpc\n",
    "import hashlib\n",
    "import os\n",
    "\n",
    "FRPC_VERSION = \"0.61.0\"\n",
    "FRPC_SHA256 = \"720a9fe2a3299346572544909a78c023344c88bde13c55b921e298e8c5ded21f\"\n",
    "FRPC_FILE = f\"frp_{FRPC_VERSION}_linux_amd64.tar.gz\"\n",
    "FRPC_URL = (\n",
    "    f\"https://github.com/fatedier/frp/releases/download/v{FRPC_VERSION}/{FRPC_FILE}\"\n",
    ")\n",
    "\n",
    "if not os.path.exists(\"frpc\"):\n",
    "    print(f\"Downloading frpc v{FRPC_VERSION}...\")\n",
    "    !curl -L {FRPC_URL} -o frp.tar.gz\n",
    "\n",
    "    # Verify Checksum\n",
    "    with open(\"frp.tar.gz\", \"rb\") as f:\n",
    "        file_hash = hashlib.sha256(f.read()).hexdigest()\n",
    "\n",
    "    if file_hash != FRPC_SHA256:\n",
    "        print(\n",
    "            f\"CRITICAL: Checksum verification failed! Expected {FRPC_SHA256}, got {file_hash}\"\n",
    "        )\n",
    "        os.remove(\"frp.tar.gz\")\n",
    "    else:\n",
    "        print(\"Checksum verified.\")\n",
    "        !tar -xzf frp.tar.gz\n",
    "        !cp frp_{FRPC_VERSION}_linux_amd64/frpc .\n",
    "        !chmod +x frpc\n",
    "        !rm -rf frp.tar.gz frp_{FRPC_VERSION}_linux_amd64\n",
    "\n",
    "if os.path.exists(\"frpc\"):\n",
    "    print(\"Setup complete!\")\n",
    "else:\n",
    "    print(\"Setup failed.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ui-cell"
   },
   "outputs": [],
   "source": [
    "# @title 2. Launch Aura Node UI\n",
    "import collections\n",
    "import os\n",
    "import subprocess\n",
    "import sys\n",
    "import threading\n",
    "import time\n",
    "\n",
    "import gradio as gr\n",
    "import requests\n",
    "import toml\n",
    "\n",
    "\n",
    "# Logging system to redirect stdout to Gradio\n",
    "class Logger:\n",
    "    def __init__(self):\n",
    "        # Keep the last 1000 chunks of log data\n",
    "        self.logs = collections.deque(maxlen=1000)\n",
    "        self.lock = threading.Lock()\n",
    "\n",
    "    def write(self, data):\n",
    "        with self.lock:\n",
    "            self.logs.append(data)\n",
    "\n",
    "    def get_logs(self):\n",
    "        with self.lock:\n",
    "            return \"\".join(self.logs)\n",
    "\n",
    "\n",
    "logger = Logger()\n",
    "\n",
    "\n",
    "class StreamToLogger:\n",
    "    def __init__(self, stream, logger):\n",
    "        self.stream = stream\n",
    "        self.logger = logger\n",
    "\n",
    "    def write(self, data):\n",
    "        self.stream.write(data)\n",
    "        self.logger.write(data)\n",
    "\n",
    "    def flush(self):\n",
    "        self.stream.flush()\n",
    "\n",
    "\n",
    "# Prevent duplicate wrapping\n",
    "if not isinstance(sys.stdout, StreamToLogger):\n",
    "    sys.stdout = StreamToLogger(sys.stdout, logger)\n",
    "    sys.stderr = StreamToLogger(sys.stderr, logger)\n",
    "\n",
    "\n",
    "class AuraNode:\n",
    "    def __init__(self):\n",
    "        self.ollama_process = None\n",
    "        self.frpc_process = None\n",
    "        self.status = \"Idle\"\n",
    "        self.requests_processed = 0\n",
    "        self.is_running = False\n",
    "        self.lock = threading.Lock()\n",
    "\n",
    "    def start(\n",
    "        self,\n",
    "        model,\n",
    "        name,\n",
    "        token,\n",
    "        remote_port,\n",
    "        server_addr,\n",
    "        server_port,\n",
    "        progress=None,\n",
    "    ):\n",
    "        if progress is None:\n",
    "            progress = gr.Progress()\n",
    "        with self.lock:\n",
    "            if self.is_running:\n",
    "                return \"Already running\"\n",
    "            self.is_running = True\n",
    "            self.status = \"Processing\"\n",
    "\n",
    "        try:\n",
    "            # 1. Start Ollama\n",
    "            progress(0, desc=\"Starting Ollama server...\")\n",
    "            print(\"--- Starting Ollama server ---\")\n",
    "            if self.ollama_process is None:\n",
    "                self.ollama_process = subprocess.Popen(\n",
    "                    [\"ollama\", \"serve\"],\n",
    "                    stdout=subprocess.PIPE,\n",
    "                    stderr=subprocess.STDOUT,\n",
    "                    text=True,\n",
    "                )\n",
    "                threading.Thread(\n",
    "                    target=self._read_output, args=(self.ollama_process,), daemon=True\n",
    "                ).start()\n",
    "\n",
    "            # Wait for ollama to be up\n",
    "            for i in range(30):\n",
    "                try:\n",
    "                    requests.get(\"http://localhost:11434\")\n",
    "                    break\n",
    "                except requests.exceptions.RequestException:\n",
    "                    time.sleep(1)\n",
    "                    progress(i / 30, desc=f\"Waiting for Ollama ({i}s)\")\n",
    "            else:\n",
    "                self.stop()  # Cleanup\n",
    "                with self.lock:\n",
    "                    self.status = \"Error: Ollama failed to start\"\n",
    "                return \"Ollama failed to start\"\n",
    "\n",
    "            # 2. Pull model\n",
    "            progress(0.1, desc=f\"Pulling model {model}...\")\n",
    "            print(f\"--- Pulling model: {model} ---\")\n",
    "            pull_proc = subprocess.Popen(\n",
    "                [\"ollama\", \"pull\", model],\n",
    "                stdout=subprocess.PIPE,\n",
    "                stderr=subprocess.STDOUT,\n",
    "                text=True,\n",
    "            )\n",
    "            for line in pull_proc.stdout:\n",
    "                print(line, end=\"\")\n",
    "            pull_proc.wait()\n",
    "\n",
    "            if pull_proc.returncode != 0:\n",
    "                self.stop()  # Cleanup\n",
    "                with self.lock:\n",
    "                    self.status = f\"Error: Failed to pull {model}\"\n",
    "                return f\"Failed to pull {model}\"\n",
    "\n",
    "            # 3. Start frpc\n",
    "            progress(0.9, desc=\"Connecting to Hive via frpc...\")\n",
    "            self._start_frpc(name, token, remote_port, server_addr, server_port)\n",
    "\n",
    "            with self.lock:\n",
    "                self.status = \"Connected\"\n",
    "            print(\"--- Aura Node is now Connected to the Hive! ---\")\n",
    "            return \"Node Connected!\"\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Error: {e}\")\n",
    "            self.stop()\n",
    "            with self.lock:\n",
    "                self.status = f\"Error: {e}\"\n",
    "            return f\"Error: {e}\"\n",
    "\n",
    "    def _start_frpc(self, name, token, remote_port, server_addr, server_port):\n",
    "        config_data = {\n",
    "            \"serverAddr\": server_addr,\n",
    "            \"serverPort\": int(server_port),\n",
    "            \"auth\": {\"method\": \"token\", \"token\": token},\n",
    "            \"proxies\": [\n",
    "                {\n",
    "                    \"name\": name,\n",
    "                    \"type\": \"tcp\",\n",
    "                    \"localIP\": \"127.0.0.1\",\n",
    "                    \"localPort\": 11434,\n",
    "                    \"remotePort\": int(remote_port),\n",
    "                }\n",
    "            ],\n",
    "        }\n",
    "\n",
    "        with open(\"frpc.toml\", \"w\") as f:\n",
    "            toml.dump(config_data, f)\n",
    "\n",
    "        print(f\"--- Starting frpc tunnel for {name} on {server_addr}:{server_port} ---\")\n",
    "        if self.frpc_process:\n",
    "            self.frpc_process.terminate()\n",
    "\n",
    "        self.frpc_process = subprocess.Popen(\n",
    "            [\"./frpc\", \"-c\", \"frpc.toml\"],\n",
    "            stdout=subprocess.PIPE,\n",
    "            stderr=subprocess.STDOUT,\n",
    "            text=True,\n",
    "        )\n",
    "        threading.Thread(\n",
    "            target=self._read_output, args=(self.frpc_process,), daemon=True\n",
    "        ).start()\n",
    "\n",
    "    def _read_output(self, process):\n",
    "        for line in process.stdout:\n",
    "            print(line, end=\"\")\n",
    "            # Increment stats on successful inference requests\n",
    "            with self.lock:\n",
    "                if \"POST /api/generate\" in line or \"POST /api/chat\" in line:\n",
    "                    self.requests_processed += 1\n",
    "                if \"login to server success\" in line:\n",
    "                    self.status = \"Connected\"\n",
    "\n",
    "    def stop(self):\n",
    "        print(\"--- Stopping Aura Node ---\")\n",
    "        if self.frpc_process:\n",
    "            self.frpc_process.terminate()\n",
    "            self.frpc_process = None\n",
    "        if self.ollama_process:\n",
    "            self.ollama_process.terminate()\n",
    "            self.ollama_process = None\n",
    "\n",
    "        # Clean up config file to prevent token leakage\n",
    "        if os.path.exists(\"frpc.toml\"):\n",
    "            try:\n",
    "                os.remove(\"frpc.toml\")\n",
    "            except OSError:\n",
    "                pass\n",
    "\n",
    "        with self.lock:\n",
    "            self.is_running = False\n",
    "            self.status = \"Idle\"\n",
    "        return \"Node stopped\"\n",
    "\n",
    "\n",
    "node = AuraNode()\n",
    "\n",
    "with gr.Blocks(title=\"Aura Node\", theme=gr.themes.Soft()) as demo:\n",
    "    gr.Markdown(\"# üêù Aura Interactive Worker Node\")\n",
    "    gr.Markdown(\n",
    "        \"**Disclaimer:** For Research & Development Use Only. Do not use for commercial hosting.\"\n",
    "    )\n",
    "\n",
    "    with gr.Row():\n",
    "        with gr.Column(scale=1):\n",
    "            status_label = gr.Label(value=node.status, label=\"Brain Status\")\n",
    "            stats_box = gr.Number(\n",
    "                value=node.requests_processed, label=\"Requests Processed\"\n",
    "            )\n",
    "        with gr.Column(scale=2):\n",
    "            with gr.Row():\n",
    "                model_input = gr.Textbox(value=\"mistral\", label=\"Ollama Model\")\n",
    "                worker_name_input = gr.Textbox(\n",
    "                    value=\"aura-worker-colab\", label=\"Worker Name\"\n",
    "                )\n",
    "            with gr.Row():\n",
    "                server_addr_input = gr.Textbox(\n",
    "                    value=\"aura.zae.life\", label=\"FRP Server Address\"\n",
    "                )\n",
    "                server_port_input = gr.Textbox(value=\"7000\", label=\"FRP Server Port\")\n",
    "            with gr.Row():\n",
    "                remote_port_input = gr.Textbox(value=\"8083\", label=\"Remote Port\")\n",
    "                token_input = gr.Textbox(label=\"Hive FRP Token\", type=\"password\")\n",
    "\n",
    "    with gr.Row():\n",
    "        start_btn = gr.Button(\"Start Node\", variant=\"primary\")\n",
    "        stop_btn = gr.Button(\"Stop Node\", variant=\"stop\")\n",
    "\n",
    "    log_output = gr.Textbox(lines=15, label=\"Agent Thinking / Logs\", interactive=False)\n",
    "\n",
    "    # Refresh logic for UI\n",
    "    timer = gr.Timer(2)\n",
    "\n",
    "    def refresh():\n",
    "        with node.lock:\n",
    "            if node.is_running:\n",
    "                # Check if processes are still alive\n",
    "                if node.ollama_process and node.ollama_process.poll() is not None:\n",
    "                    node.status = \"Error: Ollama process stopped\"\n",
    "                    node.is_running = False\n",
    "                elif node.frpc_process and node.frpc_process.poll() is not None:\n",
    "                    node.status = \"Error: frpc process stopped\"\n",
    "                    node.is_running = False\n",
    "\n",
    "            status = node.status\n",
    "            requests_processed = node.requests_processed\n",
    "        logs = logger.get_logs()\n",
    "        return status, requests_processed, logs\n",
    "\n",
    "    timer.tick(refresh, outputs=[status_label, stats_box, log_output])\n",
    "\n",
    "    start_btn.click(\n",
    "        node.start,\n",
    "        inputs=[\n",
    "            model_input,\n",
    "            worker_name_input,\n",
    "            token_input,\n",
    "            remote_port_input,\n",
    "            server_addr_input,\n",
    "            server_port_input,\n",
    "        ],\n",
    "        outputs=[log_output],\n",
    "    )\n",
    "    stop_btn.click(node.stop, outputs=[log_output])\n",
    "\n",
    "demo.launch(share=True, inline=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
